{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5445432-0a15-422b-aaf8-c6110d8a1030",
   "metadata": {},
   "source": [
    "#### Standalone Class to Reserve, Clone and Log --numstat one repository at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7dab4a-e1e2-495d-83cc-661fd2fb85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25T18:45:16.782369 Disk free: 150586.05859375 MB\n",
      "2022-05-25T18:45:16.782565 Processing cache-token matic.js\n",
      "2022-05-25T18:45:20.328560 Repo cloned/pulled\n",
      "2022-05-25T18:45:20.329055 Generating Stats for ./repos/cache-token/matic.js\n",
      "2022-05-25T18:45:24.879212 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:45:24.882354 cleaning up repo directory cache-token matic.js\n",
      "2022-05-25T18:45:24.894365 DONE cleaning up repo directory cache-token matic.js\n",
      "2022-05-25T18:45:24.983866 Disk free: 150588.5546875 MB\n",
      "2022-05-25T18:45:24.983964 Processing JaneaSystems nodejs-mobile\n",
      "2022-05-25T18:46:15.450251 Repo cloned/pulled\n",
      "2022-05-25T18:46:15.450392 Generating Stats for ./repos/JaneaSystems/nodejs-mobile\n",
      "2022-05-25T18:47:34.461535 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:47:34.464443 cleaning up repo directory JaneaSystems nodejs-mobile\n",
      "2022-05-25T18:47:35.372220 DONE cleaning up repo directory JaneaSystems nodejs-mobile\n",
      "2022-05-25T18:47:35.484591 Disk free: 150583.1875 MB\n",
      "2022-05-25T18:47:35.484702 Processing hmeine pdfdecanter\n",
      "2022-05-25T18:47:36.580340 Repo cloned/pulled\n",
      "2022-05-25T18:47:36.580826 Generating Stats for ./repos/hmeine/pdfdecanter\n",
      "2022-05-25T18:47:36.738888 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:47:36.742788 cleaning up repo directory hmeine pdfdecanter\n",
      "2022-05-25T18:47:36.744736 DONE cleaning up repo directory hmeine pdfdecanter\n",
      "2022-05-25T18:47:36.830460 Disk free: 150583.0 MB\n",
      "2022-05-25T18:47:36.830639 Processing he7d3r mw-gadget-TemplateScript\n",
      "2022-05-25T18:47:37.715391 Repo cloned/pulled\n",
      "2022-05-25T18:47:37.715513 Generating Stats for ./repos/he7d3r/mw-gadget-TemplateScript\n",
      "2022-05-25T18:47:37.852405 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:47:37.855256 cleaning up repo directory he7d3r mw-gadget-TemplateScript\n",
      "2022-05-25T18:47:37.856978 DONE cleaning up repo directory he7d3r mw-gadget-TemplateScript\n",
      "2022-05-25T18:47:37.950743 Disk free: 150582.8359375 MB\n",
      "2022-05-25T18:47:37.950926 Processing manifoldfinance metamask-debug\n",
      "2022-05-25T18:47:50.846858 Repo cloned/pulled\n",
      "2022-05-25T18:47:50.847320 Generating Stats for ./repos/manifoldfinance/metamask-debug\n",
      "2022-05-25T18:48:05.461419 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:48:05.464383 cleaning up repo directory manifoldfinance metamask-debug\n",
      "2022-05-25T18:48:05.549306 DONE cleaning up repo directory manifoldfinance metamask-debug\n",
      "2022-05-25T18:48:05.640374 Disk free: 150567.68359375 MB\n",
      "2022-05-25T18:48:05.640476 Processing yue node\n",
      "2022-05-25T18:48:52.526086 Repo cloned/pulled\n",
      "2022-05-25T18:48:52.526588 Generating Stats for ./repos/yue/node\n",
      "2022-05-25T18:50:16.474314 storing job for copying data to database in, well, the database\n",
      "2022-05-25T18:50:16.478064 cleaning up repo directory yue node\n",
      "2022-05-25T18:50:17.409436 DONE cleaning up repo directory yue node\n",
      "2022-05-25T18:50:17.522267 Disk free: 150557.4765625 MB\n",
      "2022-05-25T18:50:17.522376 Processing MateuszGroth nodev13-pcc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 186>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    184\u001B[0m                 \u001B[38;5;28mprint\u001B[39m(datingdays\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDONE cleaning up repo directory\u001B[39m\u001B[38;5;124m'\u001B[39m, owner, repo_name)\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 187\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    178\u001B[0m             \u001B[38;5;28mprint\u001B[39m(datingdays\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDisk free:\u001B[39m\u001B[38;5;124m'\u001B[39m, free, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 179\u001B[0m             numstat_req_set \u001B[38;5;241m=\u001B[39m \u001B[43mcloner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather_stats_for_repo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mowner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepo_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;66;03m#            cloner.store_results_to_database(owner, repo_name, numstat_req_set)\u001B[39;00m\n\u001B[1;32m    181\u001B[0m             cloner\u001B[38;5;241m.\u001B[39mstore_marker_for_secondary_thread(owner, repo_name)\n",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36mCloner.gather_stats_for_repo\u001B[0;34m(self, owner, repo_name)\u001B[0m\n\u001B[1;32m    121\u001B[0m last_date \u001B[38;5;241m=\u001B[39m datingdays\u001B[38;5;241m.\u001B[39mfromisoformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1972-12-26T03:23:01.123456-07:00\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    123\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://github.com/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mowner\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mrepo_name\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.git\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 124\u001B[0m cache_date \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone_pull_repo\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupdate_repo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson_stats_file_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m rep \u001B[38;5;241m=\u001B[39m Git(repo_path)\n\u001B[1;32m    127\u001B[0m need_stats \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_if_updates_are_necessary(cache_date, rep)\n",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36mCloner.clone_pull_repo\u001B[0;34m(self, url, repo_path, update_repo, json_stats_file_name)\u001B[0m\n\u001B[1;32m     70\u001B[0m cache_date \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m update_repo:\n\u001B[0;32m---> 72\u001B[0m     \u001B[43mRepo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrepo_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     rp \u001B[38;5;241m=\u001B[39m Repo(repo_path)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/git/repo/base.py:1148\u001B[0m, in \u001B[0;36mRepo.clone_from\u001B[0;34m(cls, url, to_path, progress, env, multi_options, **kwargs)\u001B[0m\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1147\u001B[0m     git\u001B[38;5;241m.\u001B[39mupdate_environment(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39menv)\n\u001B[0;32m-> 1148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_clone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mGitCmdObjectDB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/git/repo/base.py:1081\u001B[0m, in \u001B[0;36mRepo._clone\u001B[0;34m(cls, git, url, path, odb_default_type, progress, multi_options, **kwargs)\u001B[0m\n\u001B[1;32m   1078\u001B[0m     handle_process_output(proc, \u001B[38;5;28;01mNone\u001B[39;00m, to_progress_instance(progress)\u001B[38;5;241m.\u001B[39mnew_message_handler(),\n\u001B[1;32m   1079\u001B[0m                           finalize_process, decode_streams\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1081\u001B[0m     (stdout, stderr) \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1082\u001B[0m     cmdline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(proc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1083\u001B[0m     cmdline \u001B[38;5;241m=\u001B[39m remove_password_if_present(cmdline)\n",
      "File \u001B[0;32m/usr/lib/python3.8/subprocess.py:1028\u001B[0m, in \u001B[0;36mPopen.communicate\u001B[0;34m(self, input, timeout)\u001B[0m\n\u001B[1;32m   1025\u001B[0m     endtime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1028\u001B[0m     stdout, stderr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_communicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[1;32m   1031\u001B[0m     \u001B[38;5;66;03m# See the detailed comment in .wait().\u001B[39;00m\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.8/subprocess.py:1868\u001B[0m, in \u001B[0;36mPopen._communicate\u001B[0;34m(self, input, endtime, orig_timeout)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timeout(endtime, orig_timeout,\n\u001B[1;32m   1862\u001B[0m                         stdout, stderr,\n\u001B[1;32m   1863\u001B[0m                         skip_check_and_raise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1864\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(  \u001B[38;5;66;03m# Impossible :)\u001B[39;00m\n\u001B[1;32m   1865\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1866\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfailed to raise TimeoutExpired.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1868\u001B[0m ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001B[39;00m\n\u001B[1;32m   1872\u001B[0m \u001B[38;5;66;03m# objects; they are no longer using C stdio!\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.8/selectors.py:415\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    413\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 415\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import mariadb\n",
    "import time\n",
    "import hashlib\n",
    "from git import Repo, Git\n",
    "local_lib_dir = '../../../python/lib/'\n",
    "sys.path.append(local_lib_dir)\n",
    "from datetime import datetime as datingdays\n",
    "from kitchen_sink_class import Query\n",
    "from kitchen_sink_class import load_single_line_from_file as load_it\n",
    "from kitchen_sink_class import RepoName\n",
    "from commit_log_parser import NumstatRequirementSet\n",
    "from os.path import exists\n",
    "from shutil import rmtree\n",
    "from shutil import disk_usage\n",
    "\n",
    "class Cloner:\n",
    "    def __init__(self):\n",
    "        self.repo_base_dir = './repos'\n",
    "        self.result_base_dir = './results'\n",
    "        self.make_dir(self.repo_base_dir)\n",
    "        self.make_dir(self.result_base_dir)\n",
    "        self.machine_name = os.uname().nodename\n",
    "        self.database = None\n",
    "        self.db_config = None\n",
    "    def make_dir(self, dirName):\n",
    "        previously_existed = os.path.exists(dirName) and os.path.isdir(dirName)\n",
    "        if os.path.isdir(dirName) == False and os.path.exists(dirName) == False:\n",
    "            os.makedirs(dirName)\n",
    "        return previously_existed\n",
    "    def establish_dirs(self, owner, repo_name):\n",
    "        repo_dir = self.repo_base_dir+'/'+owner+'/'+repo_name\n",
    "        rslt_dir = self.result_base_dir+'/'+owner+'/'+repo_name\n",
    "        update   = self.make_dir(repo_dir)\n",
    "        self.make_dir(rslt_dir)\n",
    "        return (repo_dir, rslt_dir, update)\n",
    "    def cleanup(self, owner, repo_name):\n",
    "        rmtree(self.repo_base_dir+'/'+owner+'/'+repo_name)\n",
    "        rmtree(self.repo_base_dir+'/'+owner)\n",
    "    def load_db_info(self):\n",
    "        if self.db_config is None:\n",
    "            with open('./db.cfg', 'r') as r:\n",
    "                self.db_config = json.load(r);\n",
    "    def get_cursor(self):\n",
    "        if self.database is None:\n",
    "            self.load_db_info()\n",
    "            self.database = mariadb.connect(\n",
    "                port=self.db_config['port'],\n",
    "                host=self.db_config['host'],\n",
    "                user=self.db_config['user'],\n",
    "                password=self.db_config['password'],\n",
    "                database=self.db_config['database'],\n",
    "                autocommit=(self.db_config['autocommit']=='true'))\n",
    "            self.cursor = self.database.cursor()\n",
    "        return self.cursor\n",
    "    def reserve_next_repo(self):\n",
    "        owner = None\n",
    "        repo_name = None\n",
    "        \n",
    "        self.get_cursor()\n",
    "        self.cursor.callproc('ReserveNextRepo', (self.machine_name, None, None))\n",
    "        if self.cursor.sp_outparams: #one or more inline results set ready\n",
    "            rslt = self.cursor.fetchone()\n",
    "            owner = rslt[0]\n",
    "            repo_name = rslt[1]\n",
    "        return owner, repo_name\n",
    "    def clone_pull_repo(self, url, repo_path, update_repo, json_stats_file_name):\n",
    "        cache_date = None\n",
    "        if not update_repo:\n",
    "            Repo.clone_from(url, repo_path)\n",
    "        else:\n",
    "            rp = Repo(repo_path)\n",
    "            remote = rp.remote()\n",
    "            remote.pull()\n",
    "            if exists(json_stats_file_name):\n",
    "                try:\n",
    "                    cache_date = os.path.getmtime(json_stats_file_name)\n",
    "                    with open(json_stats_file_name) as j:\n",
    "                        numstat_req_set.resultArray = json.load(j)\n",
    "                except Exception as e:\n",
    "                    cache_date = None\n",
    "                    print(datingdays.now().isoformat(),'Error encountered trying to parse', json_stats_file_name, e)\n",
    "        print(datingdays.now().isoformat(),'Repo cloned/pulled')\n",
    "        return cache_date\n",
    "        \n",
    "    def check_if_updates_are_necessary(self, cache_date, rep):\n",
    "        need_stats = True\n",
    "        if cache_date is not None:\n",
    "            system_tz = timezone(time.tzname[0])        \n",
    "            then = datingdays.now(system_tz)\n",
    "            file_date = datingdays.fromtimestamp(cache_date, tz=system_tz)\n",
    "\n",
    "            # Add call to rep.log('-1') to get the date from the latest change\n",
    "            #  If that date is less than the date on the cached stats file\n",
    "            #  then skip this one by loading the previous stats file.\n",
    "            info = rep.log('-1')\n",
    "            for n in info.splitlines():\n",
    "                prefix = 'Date: '\n",
    "                if n.startswith(prefix):\n",
    "                    new_date = n[len(prefix):].strip()\n",
    "                    dt = datingdays.strptime(new_date, '%a %b %d %H:%M:%S %Y %z')\n",
    "                    then = then - (then - dt)\n",
    "                    print(datingdays.now().isoformat(),file_date.isoformat(), 'Last stats run')\n",
    "                    print(datingdays.now().isoformat(),then.isoformat(), 'Last Git Modification')\n",
    "                    if then < file_date:\n",
    "                        need_stats = False\n",
    "\n",
    "                #print(info)\n",
    "                #Parse the line that starts with Date\n",
    "                #Date:   Mon May 16 19:14:08 2022 +0200\n",
    "        return need_stats\n",
    "        \n",
    "    def gather_stats_for_repo(self, owner, repo_name):\n",
    "        repo = RepoName(owner, repo_name)\n",
    "        print(datingdays.now().isoformat(),'Processing', owner, repo_name)\n",
    "        repo_path, result_path, update_repo = self.establish_dirs(owner, repo_name)\n",
    "        json_stats_file_name = result_path+'/commit_stat_log.json'\n",
    "        numstat_req_set = NumstatRequirementSet()\n",
    "        last_date = datingdays.fromisoformat('1972-12-26T03:23:01.123456-07:00')\n",
    "    \n",
    "        url = 'https://github.com/'+owner+'/'+repo_name+'.git'\n",
    "        cache_date = self.clone_pull_repo(url, repo_path, update_repo, json_stats_file_name)\n",
    "\n",
    "        rep = Git(repo_path)\n",
    "        need_stats = self.check_if_updates_are_necessary(cache_date, rep)\n",
    "\n",
    "        if need_stats:        \n",
    "            print(datingdays.now().isoformat(),'Generating Stats for '+repo_path)\n",
    "            try:\n",
    "                stat = rep.log('--numstat')\n",
    "                numstat_req_set.processDocument(stat)\n",
    "            except Exception as e:\n",
    "                print('Unable to generate statistics on:',repo_path,'due to',e)\n",
    "        else:\n",
    "            print(datingdays.now().isoformat(),'Skipping',repo_path,'no changes found.')\n",
    "\n",
    "        with open(json_stats_file_name, 'w') as out:\n",
    "            out.write(json.dumps(numstat_req_set.resultArray, indent=2))\n",
    "        return numstat_req_set\n",
    "    \n",
    "    def store_results_to_database(self, owner, repo_name, numstat_req_set):\n",
    "        print(datingdays.now().isoformat(), 'writing commit history to database')\n",
    "        for n in numstat_req_set.resultArray:\n",
    "            self.cursor.callproc('InsertCommit', \n",
    "                                (owner,\n",
    "                                 repo_name, \n",
    "                                 n['commit'], \n",
    "                                 hashlib.md5(n['Author'].encode('utf-8')).hexdigest(),\n",
    "                                 n['Author'],\n",
    "                                 datingdays.fromisoformat(n['Date']),\n",
    "                                 n['orig_timezone'],\n",
    "                                 json.dumps(n['fileTypes']) ) )\n",
    "        print(datingdays.now().isoformat(), 'DONE writing commit history to database')\n",
    "    def store_marker_for_secondary_thread(self, owner, repo_name):\n",
    "        print(datingdays.now().isoformat(), 'storing job for copying data to database in, well, the database')\n",
    "        self.cursor.callproc('AddJobToUpdateQueue', \n",
    "                            (self.machine_name, owner, repo_name))\n",
    "        \n",
    "\n",
    "\n",
    "def main():\n",
    "    running = True\n",
    "    cloner = Cloner()\n",
    "    while running:\n",
    "        du = disk_usage('.')\n",
    "        free = du.free / (1024*1024)\n",
    "        if du.free < 10*1024*1024*1024:\n",
    "            print('Less than 10GB free:', free, 'MB waiting a bit for the disk cleaner-upper to catch up')\n",
    "            time.sleep(300) #sleep 5 minutes\n",
    "        else:\n",
    "            owner, repo_name = cloner.reserve_next_repo()\n",
    "            if owner is None:\n",
    "                print('No more repos to process.  Sleeping.')\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(datingdays.now().isoformat(), 'Disk free:', free, 'MB')\n",
    "                numstat_req_set = cloner.gather_stats_for_repo(owner, repo_name)\n",
    "    #            cloner.store_results_to_database(owner, repo_name, numstat_req_set)\n",
    "                cloner.store_marker_for_secondary_thread(owner, repo_name)\n",
    "                print(datingdays.now().isoformat(), 'cleaning up repo directory', owner, repo_name)\n",
    "                cloner.cleanup(owner, repo_name)\n",
    "                print(datingdays.now().isoformat(), 'DONE cleaning up repo directory', owner, repo_name)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3296a39f-d381-42fa-a37b-5f439ddc72d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02\n",
      "1.0183219909667969\n",
      "2.02\n",
      "2.018533945083618\n",
      "3.54\n",
      "3.5390961170196533\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "def calc_run_time(start_time):\n",
    "    return \"%0.2f\" % (dt.now().timestamp() - start_time)\n",
    "\n",
    "def calc_other_time(start_time):\n",
    "    return str(dt.now().timestamp() - start_time)\n",
    "\n",
    "start_time = dt.now().timestamp()\n",
    "time.sleep(1)\n",
    "print(calc_run_time(start_time))\n",
    "print(calc_other_time(start_time))\n",
    "time.sleep(1)\n",
    "print(calc_run_time(start_time))\n",
    "print(calc_other_time(start_time))\n",
    "time.sleep(1.5)\n",
    "print(calc_run_time(start_time))\n",
    "print(calc_other_time(start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}