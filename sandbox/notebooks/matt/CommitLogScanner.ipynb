{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed01f67c-e450-4840-a039-42dfc880ed91",
   "metadata": {},
   "source": [
    "### Load all resolved aliases into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c073908-d8ee-4e18-ad10-889272de863e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Do not continue!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m q \u001b[38;5;241m=\u001b[39m Query()\n\u001b[1;32m      7\u001b[0m q\u001b[38;5;241m.\u001b[39mpreload_alias_maps();\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo not continue!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Do not continue!"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "local_lib_dir = '../../../python/lib/'\n",
    "sys.path.append(local_lib_dir)\n",
    "from kitchen_sink_class import Query\n",
    "from kitchen_sink_class import load_single_line_from_file as load_it\n",
    "q = Query()\n",
    "q.preload_alias_maps();\n",
    "raise Exception('Do not continue!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76f93d-b069-4712-bc4d-925aed8a1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop_monitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0aa4b0-6209-4dcb-80ad-e8ce3e871081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4fff7-d839-4de2-9bd0-794ebe51e953",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hack window for playing with ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589c124-9525-40c7-8915-1d3576de4048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fi = FileInfo('ext')\n",
    "fi.textLineCount = 1644\n",
    "plusCount = 29\n",
    "minusCount = 0\n",
    "n = plusCount * 1.0\n",
    "print(n)\n",
    "n = n / (plusCount + minusCount)\n",
    "print(n)\n",
    "print(fi.textLineCount)\n",
    "n = n * (1.0 * fi.textLineCount)\n",
    "print(n)\n",
    "fi.insertCount = int(fi.textLineCount * ((plusCount * 1.0) / (plusCount + minusCount)))\n",
    "fi.deleteCount = fi.textLineCount - fi.insertCount\n",
    "print(fi.insertCount, ',', fi.deleteCount)\n",
    "'''\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851307c-57a7-4b85-a43b-3e8b7a02c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introspect(obj):\n",
    "  for func in [type, id, dir, vars, callable]:\n",
    "        print(\"%s(%s):\\t\\t%s\" % (func.__name__, introspect.__code__.co_varnames[0], func(obj)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620fe24-371a-49a7-b601-5baf58d2ee46",
   "metadata": {},
   "source": [
    "### Main Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb1f0d-afbb-43df-a77f-095836a506da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(2022, 5, 21, 22, 3, 44),)\n",
      "Found 205 files\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/polkadot-js/api/commit_stat_log.json\n",
      "https://github.com/polkadot-js/api\n",
      "Processed 6394 records for polkadot-js api\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/ApeWorX/ape/commit_stat_log.json\n",
      "https://github.com/ApeWorX/ape\n",
      "Processed 370 records for ApeWorX ape\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/mozilla/glam/commit_stat_log.json\n",
      "https://github.com/mozilla/glam\n",
      "Processed 961 records for mozilla glam\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/mozilla/zamboni/commit_stat_log.json\n",
      "https://github.com/mozilla/zamboni\n",
      "Processed 21570 records for mozilla zamboni\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/Biohazard1976/pip/commit_stat_log.json\n",
      "https://github.com/Biohazard1976/pip\n",
      "Processed 7658 records for Biohazard1976 pip\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/Biohazard1976/pi/commit_stat_log.json\n",
      "https://github.com/Biohazard1976/pi\n",
      "Processed 7658 records for Biohazard1976 pi\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/trustwallet/trust-wallet-ios/commit_stat_log.json\n",
      "https://github.com/trustwallet/trust-wallet-ios\n",
      "Processed 2229 records for trustwallet trust-wallet-ios\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/trustwallet/trust-web3-provider/commit_stat_log.json\n",
      "https://github.com/trustwallet/trust-web3-provider\n",
      "Processed 94 records for trustwallet trust-web3-provider\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/python-greenlet/greenlet/commit_stat_log.json\n",
      "https://github.com/python-greenlet/greenlet\n",
      "Processed 626 records for python-greenlet greenlet\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/Harsh-Avinash/court-hero-ocean-api/commit_stat_log.json\n",
      "https://github.com/Harsh-Avinash/court-hero-ocean-api\n",
      "Processed 505 records for Harsh-Avinash court-hero-ocean-api\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/ecadlabs/taquito/commit_stat_log.json\n",
      "https://github.com/ecadlabs/taquito\n",
      "Processed 1737 records for ecadlabs taquito\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/drf-forms/drf-schema-adapter/commit_stat_log.json\n",
      "https://github.com/drf-forms/drf-schema-adapter\n",
      "Processed 422 records for drf-forms drf-schema-adapter\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/flathub/com.valvesoftware.Steam/commit_stat_log.json\n",
      "https://github.com/flathub/com.valvesoftware.Steam\n",
      "Processed 450 records for flathub com.valvesoftware.Steam\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/fnreality/asmgraph/commit_stat_log.json\n",
      "https://github.com/fnreality/asmgraph\n",
      "Processed 340 records for fnreality asmgraph\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/pdoc3/pdoc/commit_stat_log.json\n",
      "https://github.com/pdoc3/pdoc\n",
      "Processed 456 records for pdoc3 pdoc\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/bbci/wyrm/commit_stat_log.json\n",
      "https://github.com/bbci/wyrm\n",
      "Processed 497 records for bbci wyrm\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/adbenitez/mailman/commit_stat_log.json\n",
      "https://github.com/adbenitez/mailman\n",
      "Processed 4645 records for adbenitez mailman\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/FrancescoXX/free-Web3-resources/commit_stat_log.json\n",
      "https://github.com/FrancescoXX/free-Web3-resources\n",
      "Processed 375 records for FrancescoXX free-Web3-resources\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/evemorgen/SMaDA-AGH/commit_stat_log.json\n",
      "https://github.com/evemorgen/SMaDA-AGH\n",
      "Processed 238 records for evemorgen SMaDA-AGH\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/ethereum/solidity/commit_stat_log.json\n",
      "https://github.com/ethereum/solidity\n",
      "Processed 13365 records for ethereum solidity\n",
      "Processing /home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results/ethereum/ethereum-org-website/commit_stat_log.json\n",
      "https://github.com/ethereum/ethereum-org-website\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "project_root_path = '../../..'\n",
    "python_lib_path = project_root_path + '/python/lib'\n",
    "sys.path.append(python_lib_path)\n",
    "import commit_log_parser\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import mariadb\n",
    "import json\n",
    "from datetime import datetime as datingdays\n",
    "from kitchen_sink_class import load_single_line_from_file as load_it\n",
    "#from hexdump import hexdump \n",
    "\n",
    "pwd = load_it('./db.pwd')\n",
    "mydb = mariadb.connect(\n",
    "    port=13306,\n",
    "    host='127.0.0.1',\n",
    "    user='w3hacknet',\n",
    "    password=pwd,\n",
    "    database='w3hacknet',\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "cursor.execute('select now()')\n",
    "res = cursor.fetchall()\n",
    "print(res[0])\n",
    "\n",
    "def insert_result(array, owner, repo_name):\n",
    "    '''\n",
    "    CREATE PROCEDURE w3hacknet.InsertCommit(owner_id varchar(128), repo_name varchar(128), commit_id char(40), author_hash char(32), author_alias varchar(128), date datetime, orig_timezone varchar(16))\n",
    "    '''\n",
    "    for n in array:\n",
    "        cursor.callproc('w3hacknet.InsertCommit', \n",
    "                        (owner,\n",
    "                         repo_name, \n",
    "                         n['commit'], \n",
    "                         hashlib.md5(n['Author'].encode('utf-8')).hexdigest(),\n",
    "                         n['Author'],\n",
    "                         datingdays.fromisoformat(n['Date']),\n",
    "                         n['orig_timezone'],\n",
    "                         json.dumps(n['fileTypes']) ) )\n",
    "    print('Processed',len(array),'records for', owner, repo_name)\n",
    "                         \n",
    "                         \n",
    "\n",
    "fileList = []\n",
    "commit_log_parser.addFiles(fileList, '/home/matt/Projects/Web3HackerNetwork/sandbox/notebooks/matt/results');\n",
    "print('Found '+str(len(fileList))+' files');\n",
    "for n in fileList:\n",
    "    if n.fileName == 'commit_stat_log.json':\n",
    "        print('Processing', n.fullyQualified)\n",
    "        p = Path(n.fullyQualified)\n",
    "        print('https://github.com/'+p.parent.parent.name+'/'+p.parent.name)\n",
    "        with open(n.fullyQualified, 'r') as r:\n",
    "            insert_result(json.load(r), p.parent.parent.name, p.parent.name)\n",
    "        #introspect(p.parent)\n",
    "'''\n",
    "root = '/home/matt/Projects/Web3HackerNetwork/data/samples'\n",
    "log = open(root+'/commitScan.log', 'w')\n",
    "for fileClass in fileList:\n",
    "    linecount = 0\n",
    "    \n",
    "    if (fileClass.fileName != 'commit-stat.log'):\n",
    "        #print('Skipping '+fileClass.fileName)\n",
    "        a = 0\n",
    "    else:\n",
    "        fq = fileClass.fullyQualified\n",
    "        print('Processing ',fq)\n",
    "        tsvFileName = str(fq)+'.json'\n",
    "        result = open(tsvFileName, 'w')\n",
    "\n",
    "        reqSet = StatRequirementSet()\n",
    "\n",
    "        with open(str(fileClass.fullyQualified), 'r') as file:\n",
    "            log.write('Reading file: '+fileClass.fullyQualified+'\\n')\n",
    "\n",
    "            for line in file:\n",
    "                linecount += 1\n",
    "                reqSet.testline(line);\n",
    "            file.close()\n",
    "            \n",
    "        result.write(json.dumps(reqSet.resultArray, indent=2))\n",
    "        reqSet.resultArray.clear()\n",
    "        nBytes = result.tell()\n",
    "        result.close();\n",
    "        if (nBytes < 1):\n",
    "            os.remove(tsvFileName)\n",
    "\n",
    "\n",
    "        print(str(fileClass.fileName)+' : '+str(linecount)+' lines')\n",
    "'''\n",
    "print('All done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d92ba-654e-454b-aad9-9a435ae04684",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "q.thread.raise_exception('Stop logging PLEASE!')\n",
    "'''\n",
    "q.monitor_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3034f-ae3b-4f10-a3bf-589c84855b2e",
   "metadata": {},
   "source": [
    "### Testing time-zone aware date comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b242f27-9316-497b-b5e2-552b54288e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as datingdays\n",
    "from pytz import timezone\n",
    "last_date = datingdays.fromisoformat('1972-12-26T03:23:01.123456-07:00')\n",
    "nowness = datingdays.now(timezone('US/Arizona'))\n",
    "print(nowness - last_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74fc0a-4fa0-49d8-80ac-4f6e288fc467",
   "metadata": {},
   "source": [
    "### Clone/Pull all of the repositories listed in input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d32f41-41d0-4681-bd25-0af21df2c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo, Git\n",
    "\n",
    "with open('./new_repo.log', 'r') as r:\n",
    "    for l in r.readlines():\n",
    "        key = l[:-1] #Strip off carriage return\n",
    "        s = key.split('/')\n",
    "        owner = s[0]\n",
    "        repo_name = s[1]\n",
    "        full_name = key\n",
    "        print('Processing', owner, repo_name)\n",
    "        repo_base_dir = './repos'\n",
    "        repo_path = repo_base_dir+'/'+key\n",
    "\n",
    "        if (os.path.isdir(repo_base_dir) == False):\n",
    "            print('######### Cannot find '+repo_base_dir+'  Creating it!')\n",
    "            os.makedirs(repo_base_dir)\n",
    "        if (os.path.isdir(repo_base_dir+\"/\"+owner) == False):\n",
    "            os.makedirs(repo_base_dir+\"/\"+owner)\n",
    "        url = 'https://github.com/'+owner+'/'+repo_name+'.git'\n",
    "        if (os.path.isdir(repo_path) == False):\n",
    "            Repo.clone_from(url, repo_path)\n",
    "        else:\n",
    "            rp = Repo(repo_path)\n",
    "            remote = rp.remote()\n",
    "            remote.pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4f6ed-3371-46db-9752-da2ff539e397",
   "metadata": {},
   "source": [
    "### Better way of running git and gather statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812f949-a9c6-4ea5-b7e2-561b0ae44b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import json\n",
    "from datetime import datetime as datingdays\n",
    "from pytz import timezone\n",
    "from pydriller import Repository\n",
    "import sys\n",
    "project_root_path = '../../..'\n",
    "python_lib_path = project_root_path + '/python/lib'\n",
    "sys.path.append(python_lib_path)\n",
    "import commit_log_parser\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "last_date = datingdays.fromisoformat('1972-12-26T03:23:01.123456-07:00')\n",
    "\n",
    "def introspect(obj):\n",
    "  for func in [type, id, dir, vars, callable]:\n",
    "        print(\"%s(%s):\\t\\t%s\" % (func.__name__, introspect.__code__.co_varnames[0], func(obj)))\n",
    "\n",
    "class CommitStats():\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=2)    \n",
    "    def __init__(self):\n",
    "        self.commit = ''\n",
    "        self.commit_timestamp = ''\n",
    "        self.author_name = ''\n",
    "        self.author_email = ''\n",
    "        self.file_stats_dic = {}\n",
    "    def determine_file_type(self, file_name):\n",
    "        retVal = 'blank'\n",
    "        if file_name is not None and len(file_name) > 0:\n",
    "            s = file_name.split('.')\n",
    "            if len(s) > 1:\n",
    "                retVal = s[len(s)-1]\n",
    "        return retVal\n",
    "    def process_commit(self, c):\n",
    "        statPerf = 0\n",
    "        base = datingdays.now().timestamp()\n",
    "        if c.author is None or c.author.name is None or c.author.email is None:\n",
    "            print('No author found in commit: ', c.hexsha)\n",
    "            if c.parents is not None and len(c.parents) > 0:\n",
    "                print('Trying recursive for ', c.parents[0].hexsha)\n",
    "                process_commit(c.parents[0])\n",
    "        else:\n",
    "            self.commit = c.hash\n",
    "            self.author_name = c.author.name\n",
    "            self.author_email = c.author.email\n",
    "            self.commit_timestamp = datingdays.isoformat(c.committer_date)\n",
    "            for stat in c.modified_files:\n",
    "                file_type = self.determine_file_type(stat.filename)\n",
    "                if file_type not in self.file_stats_dic.keys():\n",
    "                    self.file_stats_dic[file_type] = {}\n",
    "                    self.file_stats_dic[file_type]['file_count'] = 0\n",
    "                    self.file_stats_dic[file_type]['insert_count'] = 0\n",
    "                    self.file_stats_dic[file_type]['delete_count'] = 0\n",
    "                self.file_stats_dic[file_type]['file_count'] += 1\n",
    "                i = stat.added_lines\n",
    "                d = stat.deleted_lines\n",
    "                if i == 0 and d == 0:\n",
    "                    pass\n",
    "#                    print('Working dir? '+repo.working_dir)\n",
    "                else:\n",
    "                    self.file_stats_dic[file_type]['insert_count'] += i\n",
    "                    self.file_stats_dic[file_type]['delete_count'] += d\n",
    "                \n",
    "\n",
    "file_dic = {}\n",
    "user_map = {}\n",
    "print(datingdays.now())\n",
    "master_map = {}\n",
    "with open('./new_repo.log', 'r') as r:\n",
    "    for l in r.readlines():\n",
    "        base_map = {}\n",
    "        key = l[:-1] #Strip off carriage return\n",
    "        s = key.split('/')\n",
    "        owner = s[0]\n",
    "        repo_name = s[1]\n",
    "        full_name = key\n",
    "        repo_base_dir = './repos'\n",
    "        repo_path = repo_base_dir+'/'+key\n",
    "        \n",
    "        cnt = 0\n",
    "        for commit in Repository(repo_path).traverse_commits():\n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0:\n",
    "                print(cnt,'Processing', owner, repo_name, commit.author.name, commit.author_date, commit.hash)\n",
    "            obj = CommitStats()\n",
    "            obj.process_commit(commit)\n",
    "            base_map[obj.commit] = obj\n",
    "        print(cnt, 'commits processed total')\n",
    "        with open(repo_base_dir+'/commit_stat_log.json', 'w') as w:\n",
    "            w.write(json.dumps(base_map, default=lambda o: o.__dict__, sort_keys=True, indent=2))\n",
    "        master_map[key] = base_map\n",
    "        \n",
    "with open('./master_commit_stat_log.json', 'w') as w:\n",
    "    w.write(json.dumps(master_map, default=lambda o: o.__dict__, sort_keys=True, indent=2))\n",
    "    \n",
    "'''    \n",
    "    d = c.committed_datetime\n",
    "    print(d)\n",
    "    if d > last_date:\n",
    "        last_date = d;\n",
    "#    print(c.stats.total)\n",
    "    for f in c.stats.files:\n",
    "        stat = c.stats.files[f]\n",
    "        print(f, stat['insertions'], stat['deletions'], stat['lines'])\n",
    "    \n",
    "print('Latest date: ', last_date)  \n",
    "'''\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1b4d1-2e58-4ab5-b00e-a2df14547aa5",
   "metadata": {},
   "source": [
    "### Multi-threaded status logger (experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254974d-c4bb-4db0-8aea-37fc88c3393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import logging\n",
    "class Thing():\n",
    "    def __init__(self):\n",
    "        format = \"%(asctime)s: %(message)s\"\n",
    "        logging.basicConfig(format=format, level=logging.INFO,\n",
    "                            datefmt=\"%H:%M:%S\")        \n",
    "        self.thread = threading.Thread(target=self.monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "        self.running = True\n",
    "    def stop(self):\n",
    "        self.running = false\n",
    "        self.thread.interrupt()\n",
    "    def monitor(self):\n",
    "        print('Made it here')\n",
    "        while self.running:\n",
    "            time.sleep(5)\n",
    "            logging.info(\"Still here monitoring\")\n",
    "            \n",
    "t = Thing()\n",
    "print(\"Waiting around to see if background thread works\")\n",
    "print(t.thread)\n",
    "time.sleep(30)\n",
    "t.running = False\n",
    "t.join()\n",
    "print(\"Leaving...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42236563-4c0f-4bf7-aab4-31e012c47df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
